<!DOCTYPE html>
<html lang="en-US">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2020-10-30T07:31:59+01:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<h6 class="heading"><span class="type">Paragraph</span></h6>
<p>Step <a class="xref" data-knowl="./knowl/maxalg-1-derivations.html" title="Item 1">1</a> is straightforward linear algebra. In step <a class="xref" data-knowl="./knowl/maxalg-2-eigenspace-grading.html" title="Item 2">2</a>, the grading induced by the torus \(\mathfrak{t}\) has a concrete description in terms of the fixed basis \(\mathfrak{t}\text{.}\) Namely, a basis \(\delta_1,\ldots,\delta_k\) defines an isomorphism \(\mathfrak{t}^*\to F^k\) and hence an equivalent push-forward grading over \(F^k\text{.}\) Expanding out the construction of <a class="xref" data-knowl="./knowl/lemma-gradings-induced.html" title="Lemma 2.18">Lemma 2.18</a> shows that the push-forward grading has the layers</p>
<div class="displaymath">
\begin{equation*}
V_\lambda = V_{(\lambda_1,\ldots,\lambda_k)}
= \bigcap_{i=1}^k E^{\lambda_i}_{\delta_i}\text{,}
\end{equation*}
</div>
<p data-braille="continuation">where \(E^{\lambda_i}_{\delta_i}\) is the (possibly zero) eigenspace for the eigenvalue \(\lambda_i\) of the derivation \(\delta_i\text{.}\)</p>
<span class="incontext"><a href="section-constructions.html#p-125">in-context</a></span>
</body>
</html>
